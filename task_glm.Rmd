---
title: "GLM task"
author: "Sam Clifford and Megan Verma"
date: "20/05/2022"
output: 
  html_document:
      citation_package: natbib
bibliography: "MeganVerma.bib"
biblio-style: "chicago"
link-citations: true
---

``` {r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

## Introduction

This R Markdown document is designed to get you used to working with the type of data that we'll need to do the modelling in the MSc project itself. Clone the repository and make sure you commit changes regularly.

## Loading packages

``` {r, eval = TRUE}
library(tidyverse)
```

## Data preparation

We'll use the Our World in Data COVID-19 data set. Download the full data set [here](https://ourworldindata.org/covid-vaccinations) as a CSV file and save it in a new folder called `data` within the working directory of this R Markdown file. Load this data in.

``` {r, eval = TRUE}
owid <- read_csv("data/owid_covid.csv")
```

Filter the data so that we only have the rows with the most recent date in the data set.

``` {r, eval = TRUE}
owid_latest <- filter(owid, date=="2022-05-29")
```

We also want to make sure that we only have rows from real countries, rather than aggregated values. So we'll omit any rows that begin with "OWID" in the `iso_code` field. 

``` {r, eval = TRUE}
owid_latest <- filter(owid_latest, !grepl(pattern = '^OWID', x = iso_code))
```

You should have 215 rows (as of 2022-05-19) in `owid_latest`.

## Exploratory data analysis

Using the rnaturalearth package, we'll make a map of the total COVID-19 cases per million in each country.

``` {r, eval = TRUE}
library(rnaturalearth)
library(rnaturalearthdata)

```

``` {r, eval = TRUE}

world <- ne_countries(scale = "medium", returnclass = "sf")

```

Plot the world map. If using ggplot2, `geom_sf()` will give you a plot of a simple features object (class `sf`).

``` {r, eval = TRUE}

ggplot(data = world) + geom_sf() + theme_void()

```

Make a data frame consisting of the `iso_code` and total cases per million, then merge it with the `world` data frame. You'll need to make sure you tell whatever merge/join function you use which column in each data frame contains the country code to merge on.

``` {r, eval = TRUE}
iso_cases <- tibble(iso_code = owid_latest$iso_code, 
                    total_cases_per_million = owid_latest$total_cases_per_million) 

world_with_cases <- merge (x=iso_cases, y=world, 
                           by.x = "iso_code", 
                           by.y = "iso_a3")

```

Plot the world map, filling each country by the total cases per million. You may need to transform the cases variable to improve the contrast in the plot.

``` {r, eval = TRUE}
library(sf)
#library(maps)

st_crs(world_with_cases$geometry) #CRS is WGS84 

ggplot(data = world_with_cases) +  
  geom_sf(aes(geometry= geometry, #world map geometry (polygons)
              fill=total_cases_per_million)) + #color map w/ cont. values of total cases
  scale_fill_gradient(low="yellow", high="red") + #set color fill 
  theme_bw() #theme of dark text on light background 

# # diff fill
# ggplot(data = world_with_cases$geometry) +  #world map geometry (polygons)
#   geom_sf(aes(fill= world_with_cases$total_cases_per_million)) + #color map w/ cont. values of total cases
#   scale_fill_distiller(palette= "YlGnBu") + #set color fill 
#   theme_bw() #theme of dark text on light background 



# try transforming cases variable to improve contrast 
world_with_cases <- world_with_cases %>% 
  mutate(log_total_cases= log10(total_cases_per_million))

names(world_with_cases)

# plot this 
ggplot(data = mutate(world_with_cases,
                     log_total_cases = ifelse(iso_code == "PRK", NA, log_total_cases))) +
  geom_sf(aes(geometry = geometry, #world map geometry (polygons)
              fill     = log_total_cases)) + #color map w/ cont. values of LOG total cases (except N Korea)
  scale_fill_gradient(low  = "yellow",
                      high = "red",
                      name = 'log10 cases\nper million') + #set color fill 
  theme_bw() #theme of dark text on light background 
## contrast is much better 

## another alternative is to transform the colour scale
mutate(world_with_cases,
       total_cases_per_million = ifelse(iso_code == "PRK", NA, total_cases_per_million)) %>%
  
  ggplot(data = .) + 
  geom_sf(aes(geometry = geometry, #world map geometry (polygons)
              fill     = total_cases_per_million)) + #color map w/ cont. values of LOG total cases (except N Korea)
  scale_fill_gradient(low="yellow", high="red",
                      name = 'Cases per\nmillion', 
                      trans = 'log10') + #set color fill 
  theme_bw() #theme of dark text on light background 

```



Make a scatter plot of the cases per million and per capita gross domestic product.

``` {r, eval = TRUE}
ggplot(owid_latest, aes(y = total_cases_per_million,x = gdp_per_capita)) + 
    geom_point(size=2) + 
    ylab("Total Cases per Million") + 
    xlab("GDP per capita ") +
    scale_x_log10() + #log-log scale, looks like there may be a relationship 
    scale_y_log10() +
    geom_smooth(method = 'lm') +
    geom_text(data = filter(owid_latest, iso_code == "MAC"), 
              aes(label = iso_code), hjust = 1, vjust = -1)
```

## Model fitting

Fit a logistic GLM of the total number of cases (not per million) as a function of the log of GDP. As each country has a different population, we'll need to ensure that we use the estimated population as though it was a number of "trials", $n$, and the number of cases as a number of "successes", $y$, in our GLM. This is done by specifying the left hand side of the regression formula as `cbind(y, n-y)` with appropriate variable names in place of `n` and `y`.

``` {r, eval = TRUE}
library(epiDisplay)
library(conflicted)
conflict_prefer('select', 'dplyr')
conflict_prefer('filter', 'dplyr')
conflict_prefer('alpha', 'ggplot2')
conflict_prefer('multinom', 'mgcv')


owid_latest <- mutate(owid_latest, log10gdp = log10(gdp_per_capita))

case_glm <- glm(data    = owid_latest,
                formula = cbind(total_cases, population - total_cases) ~ log10gdp,
                family  = "binomial")

case_glm <- glm(data    = owid_latest %>% mutate(p = total_cases/population),
                formula = p ~ log10gdp,
                family  = "binomial", weights = population)


# fixed by passing in data as (p,N) rather than (y, N-y)
epiDisplay::logistic.display(case_glm) #the OR is 23.45
confint(case_glm)

``` 

Alternatively, we can get a tidy data frame of the confidence intervals (and more)

``` {r, eval = TRUE}
library(broom)
tidy(case_glm, conf.int = T)

# estimate is given on the log scale 
```

Is there an association between the log of GDP and a country's case numbers?

Megan: for every 10-fold increase in gdp, there is a 23.45x increase in (odds of) total cases? 
how to interpret ORs in prevalence study? 
23.45x increase in odds of being a case for every 10-fold increase in gdp? 

Sam: For interpreting output of a logistic model we often transform the coefficient, $\beta$, and report $e^{\beta}$, an odds ratio. So rather than a `r coef(case_glm)["log10gdp"]`-fold increase we see a `r exp(coef(case_glm)["log10gdp"])`-fold one.



## Extracting Gini index from WDI

Load the WDI R package and use the `WDIsearch()` function to figure out the variable indicator in the WDI database corresponding to the Gini index [@gini1936measure].

Make a new object that contains the latest value of the Gini index variable. You may need to read the help file on `WDI()`.

``` {r, eval = TRUE}
library(WDI)
WDIsearch(string = "gini") # Gini coeff indicator is called "SI.POV.GINI"

gini_latest <- WDI(country="all", 
                   indicator= "SI.POV.GINI", 
                   latest=1, language = "en")

```

Merge this new object with the data frame you used to fit the model. Ensure that you use a merge which doesn't drop rows where there is no Gini index data.

```{r, eval = TRUE}
# iso code for WDI-derived dataset is only 2-letter code, while it's only a 3-letter code for OWID dataset 
library(countrycode)
gini_latest <- mutate(gini_latest, 
               iso_code = countrycode(sourcevar   = `iso2c`, 
                                      origin      = 'iso2c',
                                      destination = 'iso3c')) 
# kosovo doesn't have an official iso code-- what are implications for nations that are younger than datasets in RESPICAR? 

owid_gini_latest <- merge(x=gini_latest, 
                          y=owid_latest,
                          by="iso_code")
```

## Incorporating household data from UN

Download the UN Population Division's data on [Household size and composition](https://www.un.org/development/desa/pd/data/household-size-and-composition) and read it in to R, ensuring you only read the rows and columns pertaining to average household size and the relevant data source. Hint: `readxl::read_xlsx()` allows you to specify a range.

``` {r, eval = TRUE}
un_data <- readxl::read_xlsx("data/un_hh.xlsx", 
                             sheet= 4, 
                             range= "A5:E819", 
                             col_names = TRUE)

un_data
```

Make a new variable in this data frame that contains the three-letter ISO code, by converting from the numeric ISO code. Hint: the countrycode package has a function, `countrycode()`, which can do this.

``` {r, eval = TRUE}

# un_data$'ISO Code' <- as.factor(un_data$`ISO Code`)
un_data <- mutate(un_data, 
               iso_code_l = countrycode(sourcevar   = `ISO Code`, 
                                      origin      = 'iso3n',
                                      destination = 'iso3c'))
un_data
```

Filter the data frame to contain only the last non-missing value for each country. Hint: use `parse_number()` to convert from character to number, as the presence of `..` in a few cells in the data column makes it a character vector.

``` {r, eval = TRUE}
un_data <- mutate(un_data, average_hh = parse_number(`Average household size (number of members)`, na = c("", "NA")))

un_data_filt <- un_data %>% 
  group_by(`Country or area`) %>% 
   mutate(refdate = as.Date(`Reference date (dd/mm/yyyy)`, format= "%d/%m/%y"))%>% 
  filter(refdate==max(refdate)) #lost 3 countries

un_data_filt
```

Summarise the data by country (ISO code) so that where there are multiple data sources with the same (latest) date, the resulting value is the average of the average household sizes.

``` {r, eval = TRUE}
#un_data_filt$`Average household size (number of members)` <- as.numeric(un_data_filt$`Average household size (number of members)`)

un_hh_iso <- un_data_filt %>% 
  group_by(`iso_code_l`) %>% 
  summarise(mean_hh = mean(average_hh, na.rm = TRUE)) #lost 40 rows because multiple data sources
```

Make a visualisation showing average household size for each country. Ensure you have appropriate labels.

``` {r, eval = TRUE}
# plot this on map 
# first merge datasets to get geometry from world
un_hh_iso_plot <- merge(y=un_hh_iso, 
                        x=world, 
                        by.y="iso_code_l", 
                        by.x="iso_a3", 
                        all.x = TRUE) #lost 6 dependent territories (of larger countries)

# which countries get dropped?
anti_join(un_hh_iso, world, by = c("iso_code_l" = "iso_a3")) %>% 
  mutate(name = countrycode::countrycode(iso_code_l, "iso3c", "country.name"))

ggplot(data = un_hh_iso_plot) + 
  geom_sf(aes(geometry = geometry, #world map geometry (polygons)
              fill     = mean_hh)) + #color map w/ cont. values mean hh size (except N Korea)
  scale_fill_gradient(low="yellow", high="red") + #set color fill 
  theme_bw() #theme of dark text on light background 

```

Sam: We should make sure that we source a value for China's average household size.

There appear to be spatial trends. Using `countrycode()`, convert to either a UN subregion or World Bank region scheme and calculate the average household size, weighting for national population.

``` {r, eval = TRUE}
un_hh_iso <- mutate(un_hh_iso, 
               un_subregion = countrycode(sourcevar   = `iso_code_l`, 
                                      origin      = 'iso3c',
                                      destination = 'un.regionsub.name'))

un_hh_iso %>% arrange(un_subregion)

names(world)
world$iso_a3
##

un_hh_world <- merge(x    = un_hh_iso,
                     y    = world,
                     by.x = "iso_code_l", 
                     by.y = "iso_a3") 
# sam: here i'm not so worried about the merge direction as we don't want to
# carry across missing values into the calculation

un_hh_world <- un_hh_world %>% dplyr::select(iso_code_l, mean_hh, un_subregion, pop_est)
# still missing those 6 territories (166 total)


un_hh_world_summ <- un_hh_world %>% 
  group_by(`un_subregion`) %>% 
  summarise(hh_pop_reg = weighted.mean(x = mean_hh, w = pop_est, na.rm = T))

un_hh_world_summ  # no longer broken

```

## Model fitting (revisited)

Create a new data frame that contains the variables required for fitting a model where the explanatory variables are log GDP, Gini index and household size. Drop any rows where there are missing values for GDP, population, Gini index, or household size.

``` {r, eval = TRUE}
# merge UN hh size dataset w/ OWID Gini dataset (latest values for both)
mod_1df <- merge(x    = un_hh_iso, 
                 y    = owid_gini_latest, 
                 by.x = "iso_code_l",
                 by.y = "iso_code") %>% 
  dplyr::select("iso_code_l", "mean_hh", "un_subregion",
                "country", "total_cases_per_million",
                "iso2c", "SI.POV.GINI", "year") 

# merge this dataset with world dataset to get GDP, total cases per million (outcome)
mod_1df <- merge(x    = mod_1df,
                 y    = world_with_cases, 
                 by.x = c("iso_code_l", "total_cases_per_million"), 
                 by.y = c("iso_code",   "total_cases_per_million")) 

mod_1df <- mod_1df %>% 
  dplyr::select("iso_code_l","mean_hh","un_subregion","country",
                "iso2c","SI.POV.GINI","year","total_cases_per_million",
                "log_total_cases", "gdp_md_est","pop_est") 

mod_1df <- mod_1df %>% mutate(log_gdp = log10(gdp_md_est))

mod_1df <- drop_na(mod_1df, SI.POV.GINI, mean_hh) 

```

Fit the model described above.

``` {r, eval = TRUE}

mod_1df <- mod_1df %>% mutate(p = total_cases_per_million/1e6)

summary(mod_1df$p)

mod_1df <- mutate(p = pmax(0, pmin(1, p))) # truncating range to [0,1]

# mod_1df <- filter(mod_1df, p <= 1, p >= 0) #now shouldn't need this (see summary)

mod_1 <- glm(data    = mod_1df,
             formula = p ~ `log_gdp` + `mean_hh` + `SI.POV.GINI`,
             family  = "binomial", weights = pop_est)

tidy(mod_1, conf.int = T) 
glance(mod_1) #shows AIC, deviance, etc

```

Sam: Cases per million is already a per-capita measure, so don't divide it by the total population. To convert it to $p \in [0,1]$ just divide by a million.

Re-fit the earlier model that contains log GDP and Gini index using the new data set you just generated.

``` {r, eval = TRUE}

mod_1.1 <- glm(data    = mod_1df,
                formula = p ~ `log_gdp` + `SI.POV.GINI`,
                family  = "binomial", 
                weights = pop_est)

tidy(mod_1.1, conf.int = T) 
glance(mod_1.1)

```


## Model comparison

Does the addition of average household size improve model fit substantially? What makes you say so?

```{r, eval = FALSE}
# much larger AIC in the model without mean hh size, so the model that includes mean hh size is a better model 
# only interested in AICs between models of the same data (only interested in the difference)
```


What happens to the estimates of the effect of GDP and inequality when adding in household size?


## Adding in region

Fit a model that also includes the subregion variable derived through `countrycode()` before. What happens to the effects of the other covariates when this spatial information is included?

``` {r, eval = TRUE}
mod_1.2 <- glm(data    = mod_1df,
                formula = p ~ `log_gdp` + `SI.POV.GINI` + `mean_hh` + `un_subregion`, 
                family  = "binomial", 
                weights = pop_est)

tidy(mod_1.2, conf.int = T) 
glance(mod_1.2)

library(purrr)
list(`gdp mean_hh Gini` = mod_1, # replace NULL with model object name
     `gdp Gini = mod_1.1,
     `gdp mean_hh Gini UNsubregion` = mod_1.2)
     %>%
  map_df(~epiDisplay::logistic.display(.x), .id = "Model") %>%
  


# further reduction in AIC with this model
# gini makes more sense (more unequal = more cases (not negative estimate)), gdp also makes more sense (higher log gdp has fewer cases), hh size still doesn't make a lot of sense (increasing hh size= fewer cases) but this is less pronounced than before 

# make a plot including each covariate from the 3 models to show how it changes between them; don't include spatial stuff (not un subregions) 

# first make a dataframe with the 3 models
tidy_mod1 <- tidy(mod_1)
tidy_mod1.1 <- tidy(mod_1.1)
tidy_mod1.2 <- tidy(mod_1.2)

all_models_df <-  merge(x=tidy_mod1, y=tidy_mod1.1,
                          by.x="term", by.y="term", all.x = TRUE)
all_models_df <-  merge(x=all_models_df, y=tidy_mod1.2,
                          by.x="term", by.y="term", all.x = TRUE)
all_models_df <- all_models_df %>% rename(estimate_mod1= estimate.x, 
                                          std.error_mod1= std.error.x,
                                          statistic_mod1= statistic.x,
                                          p.value_mod1= p.value.x, 
                                          estimate_mod1.1= estimate.y, 
                                          std.error_mod1.1= std.error.y,
                                          statistic_mod1.1= statistic.y,
                                          p.value_mod1.1= p.value.y,
                                          estimate_mod1.2= estimate, 
                                          std.error_mod1.2= std.error,
                                          statistic_mod1.2= statistic,
                                          p.value_mod1.2= p.value)
all_models_df <- all_models_df[-c(1),]

ggplot(all_models_df, aes(x= term)) + 
    geom_point(aes(y = estimate_mod1), color="blue") +
    geom_point(aes(y = estimate_mod1.1), color="green") + 
    geom_point(aes(y = estimate_mod1.2), color="red") + 
    xlab("Variables") + 
    ylab("Coefficient Estimate") + 
    ggtitle("Model Comparison") + 
    theme(plot.title = element_text(hjust=0.5)) 
    
```

Sam: how do we know which dot is from which model?

Sam: another way to do this is to bind by rows rather than merging columns

``` {r}

# as before, but with model identifiers 
tidy_mod1   <- tidy(mod_1)   %>% mutate(Model = 'Gini + log GDP + HH size')
tidy_mod1.1 <- tidy(mod_1.1) %>% mutate(Model = 'Gini + log GDP')
tidy_mod1.2 <- tidy(mod_1.2) %>% mutate(Model = 'Gini + log GDP + HH size + subregion')

tidy_model_coefs <- bind_rows(tidy_mod1, tidy_mod1.1, tidy_mod1.2) %>%
  filter(term %in% c("log_gdp", "mean_hh", "SI.POV.GINI"))

ggplot(data = tidy_model_coefs,
       aes(x = estimate, y = term)) +
  geom_point(aes(color = Model)) +
  theme(legend.position = 'bottom')

# or

ggplot(data = tidy_model_coefs,
       aes(x = estimate, y = Model)) +
  geom_point() +
  facet_wrap(~term, scales = 'free_x', ncol = 1,
             labeller = as_labeller(c("log_gdp" = "log GDP",
                                      "mean_hh" = "Mean household size",
                                      "SI.POV.GINI" = "Gini coefficient"))) +
  geom_vline(xintercept = 0, lty = 2) +
  theme_minimal() +
  theme(axis.text.y = element_text(hjust = 0)) +
  xlab("Marginal effect")
```

## Tidying up

It may be worth saving the data object at this point as it will be reused in different models. Give it a meaningful name so that it's clear what's in it and maybe when it was created (see Data Management in @goodenough).

``` {r, eval=FALSE}

write_csv(mod_1df, "data/glmcoviddata.csv")

```

## References


<!-- https://www.taylorfrancis.com/books/mono/10.4324/9780203854501/work-integrated-learning-lesley-cooper-janice-orrell-margaret-bowden -->